Ambulant design, main objects 
=============================
Jack Jansen, 01-Aug-03.
-----------------------

Objects we need:

- Event processor. This is the mainloop plus the event/callback mechanism.
- Data Source. Think of these as "media items", they may refer to a URL
  or part of a multiplexed stream or so, and provide data to renderers and
  to the parser.
- Decoders. It may be a good idea to have a decoder class that has the
  same interface as a data source and is stacked on a real data source.
- Passive Region. This is an area of screen space (or a speaker). It is "what
  the user sees".
- Active region. This is where renderers send their data.
- Renderers. These decode data streams, handle timing and do bitblitting
  or push audio through.
- Clocks. These advance a virtual time.
- Document. The representation of a SMIL document.
- Document Parser. The parser for a document.
- Document Scheduler. This reads a document and runs it.
- Timeline. This executes a single timeline.

Objects we may need:

- Resource. Control access to scarce resources (bandwidth, number of
  open files).
  
In addition we have various auxiliary objects that are explained in
``auxobjects.txt``.

Common protocols
----------------

There are a number of (formal and informal) protocols that are shared among
multiple object classes: the `refcounting protocol` and the `active/passive
protocol`.

The `refcounting protocol` is contained in the file ``lib/refcount.h``. It needs to
be implemented only by objects that is truly shared, i.e. any object whose
lifetime is not predetermined by some other object. KK will need to
explain the protocol.

The `active/passive protocol` is contained in the ``lib/skeleton.h``
header. JJ will need to explain this protocol.

Event processor
---------------

The event processor is the heart of the system, and it needs to be thought
out pretty well as the efficiency of it will in a large part determine the
snappyness of the system. We do not tack on
to existing OS infrastructure (such as the Windows event loop) for efficiency
reasons and easier portability, each event processor has an internal thread that
actually executes the events.

Event processors are passed to active objects during their creation. At the
moment we create a new event processor object for each active object, if the
object has something corresponding to an "internal clock", otherwise we
share the event processor of the parent object.

This design has the advantage that if the active object is cleaned up we
also clean up the event processor and any outstanding events it has, but it
could lead to an explosion of threads if there are many active objects. If this
turns out to be a problem it can be solved by having the event processors share
threads (transparently to their users).
   
The event processor has a number of prioritized event queues. The exact priorities
remain TBD, here it suffices to say that things like prefetching are low priority,
drawing objects on the screen is higher, and internal scheduler activity
is higher still (because it may lead to more drawing operations that should happen
"at the same time": suppose at some point you fire *internal event 1* and *internal
event 2* which should conceptually happening instantanously. *Internal event 1* fires
*external event 1*. At this point we should execute *internal event 2* before *external
event 1*, because it may in turn give rise to more external events that should
all happen at the same time).

At the moment there is only a single clock in the system, the realtime clock, and
delay values are scalar and stored in the event processor queue entries. In the future
we will have multiple clocks, so this will need some reworking. A possible
solution is to have the event processor store only the events that are ready, and to
keep the delayed events in the clocks, with the clock passing it to the event
processor when it becomes ready to run.

The priorities also need some structuring: when a data source delivers a "data ready"
event its priority depends on whether the data is just being prefetched or the
renderer is actively waiting for it (at high priority).

The event processor API is simple: pass a callback, a delay and a priority.
There could be one extra argument (or another method) to state that the originating
call does not mind if control isn't returned to it immedeately. The scheduler could
then make a shortcut and call the callback directly (if no higher priority events
are waiting), this could be used to reduce the latency when a data source signals
data availability to a renderer.

The event processor and related objects are declared in the files ``lib/event_processor.h``,
``lib/event.h`` and ``lib/callback.h``.

Data source
-----------

A data source is the thing that gives you data. It consists of two classes: a
passive data source which is a placeholder for parameters (URL, clip-begin/clip-end and such)
and which can hold caching information. The second class is the active data
source which actually fetches data and pushes it to the consumer.

The passive data source has a call ``preroll()`` which creates the active counterpart
behind the scenes, so that a later call to ``activate()`` will return quickly.

Data sources are push-based: the client asks
for data, this call returns immedeately, and when the data is available
an event is fired. There is probably also a call that says "don't
send me the event until you have all the data that is there".

*QUESTION* in addition we could have a call read_all() which does away
with all the event processing (or at least hides it) and just dumps all data into a
buffer. Do we need this?

*QUESTION* what sort of push-based API do we want? We could have one where the
producer callback simply signals data availability, and the consumer would then read()
this data (thereby signalling to the producer that it could go get more data).
Alternatively, the producer callback could contain a reference to the data/length.
In that case, if there is more data than the consumer can handle presently, who is
responsible for keeping it? I.e. can the consumer push it back to the producer, or
will it have to do buffering?

Some data sources will be able to handle some of the extra parameters
(clip-begin/end, ignore-repeat) itself, the rest will be passed to 
the renderer to handle. 

*QUESTION* is this actually feasible? Is
there a way we could do this? Maybe there is: you pass all
such extra paremeter (lets call them "filtering parameters") to the
passive data source creation method. The active data source will handle
some of these itself, and those that it can't handle it keeps for future
reference. Then when you start actually reading from the active data source
you first ask for any unhandled filtering parameters and are given this list.
you are then responsible for implementing them.

*QUESTION*
We need to think about how to handle multiplexed data streams such as RTSP.
It could be that this is simply someone elses problem (the RTSP library),
but maybe we need multiplexed data streams too.

A decoder object gets data from a data source, does something to it (such
as decoding JPEG data) and passes it on.

	I keep flipping back-and-forth about where to do decoding: at the
	source end (as stated in the previous paragraph), in the Renderer
	object or at the sink. Comments?
	
*QUESTION* buffer management isn't touched on. Do we need a buffer type?
Or does simply malloced memory, along with strict rules for who is
responsible for freeing it at what time, suffice?

*QUESTION* we could have a subclass timed_active_datasource, for data sources
which refer to timed media. Do we need this, i.e. do we ever want to seek a
datastream? And, if we do want to seek, do we want a subclass or
simply have this funcitonality on all active datasources (to be ignored
on non-timed media)?

Region
------

A region is more-or-less a SMIL region, it has XYWH parameters, a parent
region (unless it is a toplevel window) and parameters such as background
color, transparency, etc. Rendering is not done directly to a region, but
the region is activated to return a active region object to which rendering is
done.

SMIL animation on region parameters is a tricky issue. Initially I thought
the best way to solve this was again by some activate() scheme which would
return an animated_region, but SMIL animation is a truly global effect:
while the animation is active the results are seen by everything. For
this reason I now think it's best to have the region object store
two sets of parameters (XYWH, color, etc): the default set and the animated
set. An "animation is active" bit will decide which set to use.

Whenever any of the parameters is animated the region will have to
communicate this to its data sinks, which may in turn need to communicate it
to its renderer or transition.

Subregion position, and especially animation thereof, creates more
issues. It may be a good idea to simply state that every node renders
to a subregion, but with that subragion being simply a transparent
(0, 0, 100%, 100%) subregion for most nodes. 


Active region
-------------

A active region is a data sink, where data is sent. It is invoked with a SMIL region as
parameter, but multiple sinks can share a region, in which case
double-buffering for transitions and such happens there. It has no API
for graphics or such, this is handled by the renderers using the local
graphics APIs. A active region can send a callback to the renderer for
"redraw" and "reinit". The latter is redraw on steroids: the underlying
OS window may have changed (end of transition, resize). Probably user
interaction (clicking the mouse) also needs to follow this paradigm.

	An active region used to be called a data sink. The name active region is
	more in line with the other active/passive class names.
	
In case of `fill=hold` or `fill=transition` someone has to be responsible
for holding on to the active region and its contents. I think the best solution
is to give this responsibility to the region, i.e. in these cases
ownership of the active region is passed back to the region (in stead of
the active region simply being discarded). The region will need a call
that returns this "old active region" so transitions can do their thing.
That call should probably return a active region initialized to the background
if there isn't a fill=transition in effect, to simplify the transition
code (it will always transition between two data sinks, or maybe even
with two data sinks as sources and a third, new, one as destination).

	*QUESTION*: when does something finally get shown on the screen?
	The answer to this is important for the design of what preloading
	a timeline does. If instantiating the active region makes it appear
	on the screen then preloading the timeline cannot activate the region.
	It may be better to have a `show()` method on the active renderer and
	active region objects, so we can create all the objects while preloading
	a timeline. Ideally, then, by the time the timeline object is starting
	playback it sends `show()` to the active renderers, which send `show()`
	to their active regions, which have already loaded the relevant data into
	offscreen bitmaps and need only push them on-screen. 

Renderers
---------

*NOTE*: in the current implementation renderers do not yet exist, and
active regions fullfill their role too.

A renderer is called with a data source, region and a few other
parameters (such as "ignore implicit repeat in media item"). It returns
a renderer object. The renderer is the representation in a timeline
object of the node in the SMIL document.

When the renderer is activated it activates the data source and region,
and starts rendering.

*QUESTION*: at some point someone has to decide which (active) renderer
class to use to handle the media. Its could either be done dynamically,
when the active renderer has opened the data source, or statically, at the
time the static renderer object is created. The advantage of dynamic is
that at that point we have a data source open, so we can presumably get
MIME type and such at low cost. On the other hand doing it statically means
we don't have to do it all again when we play the same media again later.
Maybe a caching scheme is in order again here? Then the question also
arises where to cache: at the data source is probably a good candidate.

The renderer activates the region to a active region, it cannot be called
with a active region as parameter (as it was in earlier versions of this document)
because renderer objects are stateless.

The renderer object interface should allow for at least three different
implementations:

1. The normal one, where the renderer depends on the data source for
   data, decodes it, and sends it to the active region.
2. A "third party" renderer, that just grabs the information from the
   data source and sink and passes it to a third party engine (think
   QuickTime or Direct X).
3. A mix of the above.

We need the dormant renderer object, which only holds the information
needed to allow later activation (on demand?). Otherwise resources may
become scarce: think of a seq with 1000 slides: we don't want to open
all images beforehand or we will run out of file descriptors. But
sometimes the document scheduler must open the data source, if the media
item is part of a switch, for instance.

Clocks
------

*NOTE*: in the current implementation clocks do not yet exist, and
there is only the global realtime clock. The interface is in ``lib/timer.h`` and
``lib/delta_timer.h``.

Clocks come in two flavors. The normal flavor is a free-running clock, a
renderer that is subscribed to it can access it in two modes (depending
on what programming model it prefers): callback or polling. In callback
mode it will get callbacks when the virtual clock time has drifted more
than delta from the wallclock. In polling mode it can ask for the time
and ask for a callback at a specific time/interval. Renderers can also
tell the clock what time they think it is, but this is completely
ignored.

The other clock flavor is the master clock. Each normal clock can have
at most one master clock attached to it. The master clock has the same
API as a normal clock, from the renderer point of view, but it does not
give callbacks and it *does* listen when the renderer tells it what time
it is (and updates its dependent slave clock). The scheduler assigns a
master clock for the SMIL syncMaster attribute.

	*NOTE* I am rethinking this section. The fact that some times a clock
	is slaved to another clock and sometimes it is slaved to a renderer needs
	to be exploited, probably through an interface for any object that can
	be master to a clock. Then all clocks will be slaved to such a clock
	master, either another clock, a renderer or the "realtime master clock"
	object.

SMIL Document tree
------------------

To be supplied by KK. Interface is in (I guess) ``lib/node.h``, ``lib/node_iterator.h``,
``lib/node_navigator.h``, ``lib/region_node.h``, ``lib/region_dim.h``, ``lib/vnode.h``.

SMIL Document parser
--------------------

To be supplied by KK. Interface is in (I guess) ``lib/expat_parser.h``,
``lib/nscontext.h`` (correct?), ``lib/parse_attrs.h``, ``lib/region_eval.h``,
``lib/sax_handler.h``, ``lib/sax_types.h`` (correct?), ``lib/smil_handler.h``.

Player
------

The player is the top-level object. It opens the document (through a data source)
and parses it into a tree. It then waits for the "play document" event.
Upon receipt it will create the timeline schedule for the root plus anything
that is on the same timeline.

The player uses an auximilary object ``timeline_builder`` that creates
the timelines for a given SMIL tree. In the current implementation, with
only MMS 2 documents supported, a single timeline suffices to implement
the schedule for a document. In the future, when we support looping
and user interaction, this will not be the case any more, and the player
object will become more complex. It will have to listen for
callbacks from timeline objects (timeline finished, user interaction)
and react by creating new timelines and starting them.

*QUESTION*: who is responsible for creating regions? The HL scheduler
seems the best candidate, but that would interfere with any decoupling
of HL and LL scheduler. Also, if we want to handle subregion positioning
by inserting a temporary region into the region tree the timeline scheduler
should also be able to create regions. Animating subregion positioning
complicates matters even more.

Timelines
---------

Timeline objects again come in active and passive flavors. The passive
flavor is what is created by the SMIL document scheduler, which is then
activated when pushed down, at which point the active timeline starts
running.

Timeline design is elaborated upon in the document ``timelines.txt``,
and the API is in ``lib/timelines.h``. 

The timelines can also be pushed down early, in which case they
will start preloading. This can be used to do "jump optimization" on
user interaction: if the scheduler can guess where the user is going to
go it can push the schedule down early thereby giving better responsiveness.

The main data structure in a timeline is the the list of (prerequisites, action)
tuples. This is almost a petri-net: when all of the prerequisites have happend
then all action are triggered. There is a difference with a real petri-net in
that some of these tuples are "or" in stead of "and": as soon as any of the
prerequisites has fired the actions fire. We need a name for these tuples (and
node and event are already taken), "place" seems to be the petri-net standard
but I don't like it... 

Actions can either trigger another prerequisite, cause an event to be
sent to a renderer (play, stop, etc), trigger a clock callback later or
do an upcall to the document scheduler. These should probably all be
done through the event processor, so we can use it's priority scheme to
do things in the right order.

The timeline has a number of auxiliary objects: 

- the renderers used (with their data sources attached), 
- the clock (or master and slave clock), 
- probably more.