Ambulant design, main objects 
=============================
30-Sep-03
---------

Objects we need:

- Event processor. This is the mainloop plus the event/callback mechanism.
- Data Source. Think of these as "media items", they may refer to a URL
  or part of a multiplexed stream or so, and provide data to renderers and
  to the parser.
- Decoders. It may be a good idea to have a decoder class that has the
  same interface as a data source and is stacked on a real data source.
- Passive Region. This is an area of screen space (or a speaker). It is "what
  the user sees".
- Active region. This is where renderers send their data.
- Renderers. These decode data streams, handle timing and do bitblitting
  or push audio through.
- Clocks. These advance a virtual time.
- Document. The representation of a SMIL document.
- Document Parser. The parser for a document.
- Document Scheduler. This reads a document and runs it.
- Timeline. This executes a single timeline.
- Player. This is the toplevel object.

Objects we may need:

- Resource. Control access to scarce resources (bandwidth, number of
  open files).
  
In addition we have various auxiliary objects that are explained in
``auxobjects.txt``.

Common protocols
----------------

There are a number of (formal and informal) protocols that are shared among
multiple object classes: the `refcounting protocol` and the `active/passive
protocol`.

The `refcounting protocol` is contained in the file ``lib/refcount.h``. It needs to
be implemented only by objects that are truly shared, i.e. any object whose
lifetime is not predetermined by some other object. New instances of refcounted
objects are created using the operator new. Any object that needs to share
a particular instance calls add_ref against this instance. The creator
of the refcounted object and any sharer are responsible to call the release
function of the object when they don't need the object any more. 

The `active/passive protocol` is more a pattern than a protocol. The pattern
is used often in Ambulant for objects that go through two stages during
their lifetime: a building stage and an active (running) stage. The passive
object is the object that corresponds to the building stage and the built
stage that follows it. The passive object *appears* to be immutable.
Once the object actually needs to do work we activate it, giving a new instance
of an active object that does the actual work. The passive object remains
in existence, and can be re-used.

Note that I say "appears immutable": passive objects could internally cache
information, or keep one or more active instances ready for quick disposal.

Event processor
---------------

The event processor is the heart of the system, and it needs to be thought
out pretty well as the efficiency of it will in a large part determine the
snappyness of the system. We do not tack on
to existing OS infrastructure (such as the Windows event loop) for efficiency
reasons and easier portability, each event processor has an internal thread that
actually executes the events.

Event processors are passed to active objects during their creation. At the
moment we create a new event processor object for each active object, if the
object has something corresponding to an "internal clock", otherwise we
share the event processor of the parent object.

This design has the advantage that if the active object is cleaned up we
also clean up the event processor and any outstanding events it has, but it
could lead to an explosion of threads if there are many active objects. If this
turns out to be a problem it can be solved by having the event processors share
threads (transparently to their users).
   
The event processor has a number of prioritized event queues. The exact priorities
remain TBD, here it suffices to say that things like prefetching are low priority,
drawing objects on the screen is higher, and internal scheduler activity
is higher still (because it may lead to more drawing operations that should happen
"at the same time": suppose at some point you fire *internal event 1* and *internal
event 2* which should conceptually happening instantanously. *Internal event 1* fires
*external event 1*. At this point we should execute *internal event 2* before *external
event 1*, because it may in turn give rise to more external events that should
all happen at the same time).

At the moment there is only a single clock in the system, the realtime clock, and
delay values are scalar and stored in the event processor queue entries. In the future
we will have multiple clocks, so this will need some reworking. A possible
solution is to have the event processor store only the events that are ready, and to
keep the delayed events in the clocks, with the clock passing it to the event
processor when it becomes ready to run.

The priorities also need some structuring: when a data source delivers a "data ready"
event its priority depends on whether the data is just being prefetched or the
renderer is actively waiting for it (at high priority).

The event processor API is simple: pass a callback, a delay and a priority.
There could be one extra argument (or another method) to state that the originating
call does not mind if control isn't returned to it immedeately. The scheduler could
then make a shortcut and call the callback directly (if no higher priority events
are waiting), this could be used to reduce the latency when a data source signals
data availability to a renderer.

The event processor and related objects are declared in the files ``lib/event_processor.h``,
``lib/event.h`` and ``lib/callback.h``.

Data source
-----------

A data source is the thing that gives you data. It consists of two classes: a
passive data source which is a placeholder for parameters (URL, clip-begin/clip-end and such)
and which can hold caching information. The second class is the active data
source which actually fetches data and pushes it to the consumer.

The passive data source has a call ``preroll()`` which creates the active counterpart
behind the scenes, so that a later call to ``activate()`` will return quickly.

Data sources are push-based: the client calls ``start()`` to ask
for data, this call returns immediately, and when the data is available
an event is fired. In the future we probably also want a variant of ``start()``
that does not fire the event as soon as *any* data is available, but only when
*all* data has been read.

The callback does not contain the data, it is only a signal of its availability.
The ``read()`` methods allows the caller to read the actual data. At the moment
there is a lot of unnecessary copying going on, this will need change in the
future.

Some data sources may be able to handle some of the extra parameters
(clip-begin/end, ignore-repeat) itself, the rest will be passed to 
the renderer to handle. At the moment there is no provision for passing
this information to the data source.

	*OPEN ISSUE*
	Is this actually feasible? Is
	there a way we could do this? Maybe there is: you pass all
	such extra pareaeter (lets call them "filtering parameters") to the
	passive data source creation method. The active data source will handle
	some of these itself, and those that it can't handle it keeps for future
	reference. Then when you start actually reading from the active data source
	you first ask for any unhandled filtering parameters and are given this list.
	you are then responsible for implementing them.
	
	*OPEN ISSUE*
	We need to think about how to handle multiplexed data streams such as RTSP.
	It could be that this is simply someone elses problem (the RTSP library),
	but maybe we need multiplexed data streams too.

	*OPEN ISSUE* we could have a subclass timed_active_datasource, for data sources
	which refer to timed media. Do we need this, i.e. do we ever want to seek a
	datastream? And, if we do want to seek, do we want a subclass or
	simply have this funcitonality on all active datasources (to be ignored
	on non-timed media)?

A decoder data source object gets data from a data source, does something to it (such
as decoding JPEG data) and passes it on.

	*OPEN ISSUE*
	 I keep flipping back-and-forth about where to do decoding: at the
	source end (as stated in the previous paragraph) or in the Renderer
	object. Current GUI toolkits can do a lot of things like decoding JPEGs...
	
Active data sources have a ``databuffer`` auxiliary object that stores the
data. The current implementation of this is a placeholder, it should be replaced
by something that is more efficient. The API will probably also change for
this.

Region
------

A region is more-or-less a SMIL region, it has XYWH parameters, a parent
region (unless it is a toplevel window) and parameters such as background
color, transparency, etc. Rendering is not done directly to a region, but
the region is activated to return a active region object to which rendering is
done. When an ``active_region`` is ready to be displayed it tells this to its
``passive_region``, which remembers it as its current active region.


``Passive_window`` is a subclass of ``passive_region`` used for toplevel
windows (``root_layout`` in SMIL terms). Passive_window objects are created
by calling the ``newwindow()`` method of a ``window_factory`` object.

These latter two objects are again subclassed for each GUI toolkit that needs
to be supported. Usually, the ``window_factory`` will contain some sort of
pointer to the underlying GUI window, and this pointer is passed along to
each ``passive_window`` created.

Each region has its own private coordinate system, with ``(0, 0)`` at the
top-left corner. Each region also stores its visible area in the coordinate
system of its parent region, and conversion between coordinate systems is
done on each upcall and downcall. In addition each region currently caches
its ``(0, 0)`` location in its root ``passive_window`` coordinate system
for quick access, but this could be changed.

Passive regions have one upcall and one downcall: ``need_redraw()`` is an
upcall signalling that an area needs to be redrawn. It is initially generated
by a renderer (usually), passed to the ``active_region``, passed up the
``passive_region`` chain to the ``passive_window``, which passes it to the
GUI toolkit. The corresponding downcall is ``redraw()``, which is generated
by the ``passive_window`` in response to a GUI toolkit event. Each ``passive_region``
passes it to its active current region (if there is one), and then passes it
down to all its child regions that overlap the region needing redraw. The active
regions pass it on to the corresponding renderers.

The ``passive_window`` is passed along to the ``redraw()`` calls, this is
how the (toolkit-dependent) renderers eventually get at a pointer to the window
to render in.

	*OPEN ISSUE*
	SMIL animation on region parameters is a tricky issue. Initially I thought
	the best way to solve this was again by some activate() scheme which would
	return an animated_region, but SMIL animation is a truly global effect:
	while the animation is active the results are seen by everything. For
	this reason I now think it's best to have the region object store
	two sets of parameters (XYWH, color, etc): the default set and the animated
	set. An "animation is active" bit will decide which set to use.

	Whenever any of the parameters is animated the region will have to
	communicate this to its data sinks, which may in turn need to communicate it
	to its renderer or transition.
	
	Subregion position, and especially animation thereof, creates more
	issues. It may be a good idea to simply state that every node renders
	to a subregion, but with that subregion being simply a transparent
	(0, 0, 100%, 100%) subregion for most nodes. 

At the moment we only have an implementation for regions that correspond
to actual screen regions, but SMIL also specifies audio regions. These
will probably be implemented with superclasses of the current classes, with
the functionality common to both visible and invisible regions moving to
the superclasses.

An active region is a data sink, where data is sent. It has no API
for graphics or such, this is handled by the renderers using the local
graphics APIs. At the moment ``redraw()`` is the only call an active region
makes to its corresponding active renderer, but this may need to change 
in the future to distinguish between simple redraws and resizes.

In case of `fill=hold` or `fill=transition` someone has to be responsible
for holding on to the active region and its contents. I think the best solution
is to give this responsibility to the passive region, i.e. in these cases
ownership of the active region is passed back to the region (in stead of
the active region simply being discarded). The region will need a call
that returns this "old active region" so transitions can do their thing.
That call should probably return a active region initialized to the background
if there isn't a fill=transition in effect, to simplify the transition
code (it will always transition between two data sinks, or maybe even
with two data sinks as sources and a third, new, one as destination).

	*OPEN ISSUE*
	At the moment nothing happens if there is no current active region
	and a passive region gets a redraw. It may be a good idea to have
	a specialized active region that shows the default background color,
	etc.
	
Renderers
---------

A renderer is called with a data source, active region and event processor.
It only exists in the active flavor currently, but the ``active_action``
sub-object of the ``active_timeline`` could be seen as its passive
counterpart.

Renderers are created through the ``new_renderer()`` method of a
``renderer_factory`` object, passing the node we want to render.
This object is subclassed for each GUI toolkit
we want to support. In addition, for each toolkit we also subclass
``active_renderer`` for each media type we support. So, to give
an example, for Cocoa we have ``cocoa_renderer_factory`` which returns
(depending on the node type) an object of any of the classes
``cocoa_active_text_renderer``, ``cocoa_active_image_renderer``, etc.

When the renderer is activated it activates the data source and starts it.
It then waits for either data to become available, in which case it sends
a ``need_redraw()`` to its region, or ``redraw()`` calls from its region,
in which case it renders what it can (the full data if available, else
maybe parital data, else a background).

In future, he renderer object interface should allow for at least three different
implementations:

1. The normal one, where the renderer depends on the data source for
   data, decodes it, and sends it to the active region.
2. A "third party" renderer, that just grabs the information from the
   data source and sink and passes it to a third party engine (think
   QuickTime or Direct X).
3. A mix of the above.

Clocks
------

*NOTE*: in the current implementation clocks do not yet exist, and
there is only the global realtime clock. The interface is in ``lib/timer.h`` and
``lib/delta_timer.h``.

Clocks come in two flavors. The normal flavor is a free-running clock, a
renderer that is subscribed to it can access it in two modes (depending
on what programming model it prefers): callback or polling. In callback
mode it will get callbacks when the virtual clock time has drifted more
than delta from the wallclock. In polling mode it can ask for the time
and ask for a callback at a specific time/interval. Renderers can also
tell the clock what time they think it is, but this is completely
ignored.

The other clock flavor is the master clock. Each normal clock can have
at most one master clock attached to it. The master clock has the same
API as a normal clock, from the renderer point of view, but it does not
give callbacks and it *does* listen when the renderer tells it what time
it is (and updates its dependent slave clock). The scheduler assigns a
master clock for the SMIL syncMaster attribute.

	*NOTE* I am rethinking this section. The fact that some times a clock
	is slaved to another clock and sometimes it is slaved to a renderer needs
	to be exploited, probably through an interface for any object that can
	be master to a clock. Then all clocks will be slaved to such a clock
	master, either another clock, a renderer or the "realtime master clock"
	object.

SMIL Document tree
------------------

To be supplied by KK. Interface is in (I guess) ``lib/node.h``, ``lib/node_iterator.h``,
``lib/node_navigator.h``, ``lib/region_node.h``, ``lib/region_dim.h``, ``lib/vnode.h``.

SMIL Document parser
--------------------

To be supplied by KK. Interface is in (I guess) ``lib/expat_parser.h``,
``lib/nscontext.h`` (correct?), ``lib/parse_attrs.h``, ``lib/region_eval.h``,
``lib/sax_handler.h``, ``lib/sax_types.h`` (correct?), ``lib/smil_handler.h``.

Player
------

The player is the top-level object. It opens the document (through a data source)
and parses it into a tree. It then waits for the "play document" event.
Upon receipt it will create the timeline schedule for the root plus anything
that is on the same timeline.

The player uses an auximilary object ``timeline_builder`` that creates
the timelines for a given SMIL tree. In the current implementation, with
only MMS 2 documents supported, a single timeline suffices to implement
the schedule for a document. In the future, when we support looping
and user interaction, this will not be the case any more, and the player
object will become more complex. It will have to listen for
callbacks from timeline objects (timeline finished, user interaction)
and react by creating new timelines and starting them.

Timelines
---------

Timeline objects again come in active and passive flavors. The passive
flavor is what is created by the SMIL document scheduler, which is then
activated when pushed down, at which point the active timeline starts
running.

Timeline design is elaborated upon in the document ``timelines.txt``,
and the API is in ``lib/timelines.h``. 

The timelines can also be pushed down early, in which case they
will start preloading. This can be used to do "jump optimization" on
user interaction: if the scheduler can guess where the user is going to
go it can push the schedule down early thereby giving better responsiveness.

The main data structure in a timeline is the the list of (prerequisites, action)
tuples. This is almost a petri-net: when all of the prerequisites have happend
then all action are triggered. There is a difference with a real petri-net in
that some of these tuples are "or" in stead of "and": as soon as any of the
prerequisites has fired the actions fire. We need a name for these tuples (and
node and event are already taken), "place" seems to be the petri-net standard
but I don't like it... 

Actions can either trigger another prerequisite, cause an event to be
sent to a renderer (play, stop, etc), trigger a clock callback later or
do an upcall to the document scheduler. These should probably all be
done through the event processor, so we can use it's priority scheme to
do things in the right order.

The timeline has a number of auxiliary objects: 

- the renderers used (with their data sources attached), 
- the clock (or master and slave clock), 
- probably more.