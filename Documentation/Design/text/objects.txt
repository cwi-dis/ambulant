Ambulant design, objects 
========================
Jack Jansen, 20-Jun-03.
-----------------------

Objects we need:

- Event processor. This is the mainloop plus the event/callback mechanism.
- Data Source. Think of these as "media items", they may refer to a URL
  or part of a multiplexed stream or so, and provide data to renderers and
  to the parser.
- Decoders. It may be a good idea to have a decoder class that has the
  same interface as a data source and is stacked on a real data source.
- Region. This is an area of screen space (or a speaker). It is "what
  the user sees".
- Data sink. This is where renderers send their data.
- Renderers. These decode data streams, handle timing and do bitblitting
  or push audio through.
- Clocks. These advance a virtual time.
- Document Scheduler. This reads a document and runs it.
- Timeline. This executes a single timeline.

Objects we may need:

- Resource. Control access to scarce resources (bandwidth, number of
  open files).

Event processor
---------------

The event processor is the heart of the system, and it needs to be thought
out pretty well as the efficiency of it will in a large part determine the
snappyness of the system. Design of this is TBD, because it depends on a number
of unknowns (such as how to represent callbacks). We may want to tack on
to existing OS infrastructure (such as the Windows event loop), but that may
be too inefficient.

*QUESTION*: is the event processor a global object? It has to be available when
an object wants to schedule a callback, so it either has to be a global object
or it has to be passed explicitly to constructor methods or when setting the
callback to call. And, if the event processor is a global object, we have the
problem that the HL scheduler also wants to use the data source, but it lives
in a different thread. Is this a problem?

Data source
-----------

A data source is the thing that gives you data. It consists of two classes: a
passive data source which is a placeholder for parameters (URL, clip-begin/clip-end and such)
and which can hold caching information. The second class is the active data
source which actually fetches data and pushes it to the consumer.

The passive data source has a call "preroll()" which creates the active counterpart
behind the scenes, so that a later call to activate() will return quickly.

Data sources are push-based: the client asks
for data, this call returns immedeately, and when the data is available
an event is fired. There is probably also a call that says "don't
send me the event until you have all the data that is there".

*QUESTION* in addition we could have a call read_all() which does away
with all the event processing (or at least hides it) and just dums all data into a
buffer. Do we need this?

*QUESTION* what sort of push-based API do we want? We could have one where the
producer callback simply signals data availability, and the consumer would then read()
this data (thereby signalling to the producer that it could go get more data).
Alternatively, the producer callback could contain a reference to the data/length.
In that case, if there is more data than the consumer can handle presently, who is
responsible for keeping it? I.e. can the consumer push it back to the producer, or
will it have to do buffering?

Some data sources will be able to handle some of the extra parameters
(clip-begin/end, ignore-repeat) itself, the rest will be passed to 
the renderer to handle. 

*QUESTION* is this actually feasible? Is
there a way we could do this? Maybe there is: you pass all
such extra paremeter (lets call them "filtering parameters") to the
passive data source creation method. The active data source will handle
some of these itself, and those that it can't handle it keeps for future
reference. Then when you start actually reading from the active data source
you first ask for any unhandled filtering parameters and are given this list.
you are then responsible for implementing them.

*QUESTION* there
We need to think about how to handle multiplexed data streams such as RTSP.
It could be that this is simply someone elses problem (the RTSP library),
but maybe we need multiplexed data streams too.

A decoder object gets data from a data source, does something to it (such
as decoding JPEG data) and passes it on.

	I keep flipping back-and-forth about where to do decoding: at the
	source end (as stated in the previous paragraph), in the Renderer
	object or at the sink. Comments?
	
*QUESTION* buffer management isn't touched on. Do we need a buffer type?
Or does simply malloced memory, along with strict rules for who is
responsible for freeing it at what time, suffice?

*QUESTION* we could have a subclass timed_active_datasource, for data sources
which refer to timed media. Do we need this, i.e. do we ever want to seek a
datastream? And, if we do want to seek, do we want a subclass or
simply have this funcitonality on all active datasources (to be ignored
on non-timed media)?

Region
------

A region is more-or-less a SMIL region, it has XYWH parameters, a parent
region (unless it is a toplevel window) and parameters such as background
color, transparency, etc. Rendering is not done directly to a region, but
the region is activated to return a data sink object to which rendering is
done.

SMIL animation on region parameters is a tricky issue. Initially I thought
the best way to solve this was again by some activate() scheme which would
return an animated_region, but SMIL animation is a truly global effect:
while the animation is active the results are seen by everything. For
this reason I now think it's best to have the region object store
two sets of parameters (XYWH, color, etc): the default set and the animated
set. An "animation is active" bit will decide which set to use.

Whenever any of the parameters is animated the region will have to
communicate this to its data sinks, which may in turn need to communicate it
to its renderer or transition.

Subregion position, and especially animation thereof, creates more
issues. It may be a good idea to simply state that every node renders
to a subregion, but with that subragion being simply a transparent
(0, 0, 100%, 100%) subregion for most nodes. 


Data sink
---------

A data sink is where data is sent. It is invoked with a SMIL region as
parameter, but multiple sinks can share a region, in which case
double-buffering for transitions and such happens there. It has no API
for graphics or such, this is handled by the renderers using the local
graphics APIs. A data sink can send a callback to the renderer for
"redraw" and "reinit". The latter is redraw on steriods: the underlying
OS window may have changed (end of transition, resize). Probably user
interaction (clicking the mouse) also needs to follow this paradigm.

A better name for data sink may be active region (analogous to data 
source/active data source).

In case of fill=hold or fill=transition someone has to be responsible
for holding on to the data sink and its contents. I think the best solution
is to give this responsibility to the region, i.e. in these cases
ownership of the data sink is passed back to the region (in stead of
the data sink simply being discarded). The region will need a call
that returns this "old data sink" so transitions can do their thing.
That call should probably return a data sink initialized to the background
if there isn't a fill=transition in effect, to simplify the transition
code (it will always transition between two data sinks, or maybe even
with two data sinks as sources and a third, new, one as destination).

Renderers
---------

A renderer is called with a data source, region and a few other
parameters (such as "ignore implicit repeat in media item"). It returns
a renderer object. The renderer is the representation in a timeline
object of the node in the SMIL document.

When the renderer is activated it activates the data source and region,
and starts rendering.

*QUESTION*: at some point someone has to decide which (active) renderer
class to use to handle the media. Its could either be done dynamically,
when the active renderer has opened the data source, or statically, at the
time the static renderer object is created. The advantage of dynamic is
that at that point we have a data source open, so we can presumably get
MIME type and such at low cost. On the other hand doing it statically means
we don't have to do it all again when we play the same media again later.
Maybe a caching scheme is in order again here? Then the question also
arises where to cache: at the data source is probably a good candidate.

The renderer activates the region to a data sink, it cannot be called
with a data sink as parameter (as it was in earlier versions of this document)
because renderer objects are stateless.

The renderer object interface should allow for at least three different
implementations:

1. The normal one, where the renderer depends on the data source for
   data, decodes it, and sends it to the data sink.
2. A "third party" renderer, that just grabs the information from the
   data source and sink and passes it to a third party engine (think
   QuickTime or Direct X).
3. A mix of the above.

We need the dormant renderer object, which only holds the information
needed to allow later activation (on demand?). Otherwise resources may
become scarce: think of a seq with 1000 slides: we don't want to open
all images beforehand or we will run out of file descriptors. But
sometimes the document scheduler must open the data source, if the media
item is part of a switch, for instance.

Clocks
------

Clocks come in two flavors. The normal flavor is a free-running clock, a
renderer that is subscribed to it can access it in two modes (depending
on what programming model it prefers): callback or polling. In callback
mode it will get callbacks when the virtual clock time has drifted more
than delta from the wallclock. In polling mode it can ask for the time
and ask for a callback at a specific time/interval. Renderers can also
tell the clock what time they think it is, but this is completely
ignored.

The other clock flavor is the master clock. Each normal clock can have
at most one master clock attached to it. The master clock has the same
API as a normal clock, from the renderer point of view, but it does not
give callbacks and it *does* listen when the renderer tells it what time
it is (and updates its dependent slave clock). The scheduler assigns a
master clock for the SMIL syncMaster attribute.

SMIL Document tree
------------------

The SMIL Document Scheduler opens the document (through a data source)
and parses it into a tree. It then waits for the "play document" event.
Upon receipt it will create the timeline schedule for the root plus anything
that is on the same timeline.


*This section is wrong*: It creates the data sources and renderers,
pushes the whole timeline down and fires the event that sets the thing running.
The timeline schedule also contains preloads for the various media items.
Then it waits for more events (probably from the LL scheduler) and does
what is needed for them.

*QUESTION*: who is responsible for creating regions? The HL scheduler
seems the best candidate, but that would interfere with any decoupling
of HL and LL scheduler. Also, if we want to handle subregion positioning
by inserting a temporary region into the region tree the timeline scheduler
should also be able to create regions. Animating subregion positioning
complicates matters even more.

Timelines
---------

Timeline objects again come in active and passive flavors. The passive
flavor is what is created by the SMIL document scheduler, which is then
activated when pushed down, at which point the active timeline starts
running.

The timelines can also be pushed down early, in which case they
will start preloading. This can be used to do "jump optimization" on
user interaction: if the scheduler can guess where the user is going to
go it can push the schedule down early thereby giving better responsiveness.

The main data structure in a timeline is the the list of (prerequisites, action)
tuples. This is almost a petri-net: when all of the prerequisites have happend
then all action are triggered. There is a difference with a real petri-net in
that some of these tuples are "or" in stead of "and": as soon as any of the
prerequisites has fired the actions fire. We need a name for these tuples (and
node and event are already taken), "place" seems to be the petri-net standard
but I don't like it... 

Actions can either trigger another prerequisite, cause an event to be
sent to a renderer (play, stop, etc), trigger a clock callback later or
do an upcall to the document scheduler. These should probably all be
done through the event processor, so we can use it's priority scheme to
do things in the right order.

The timeline has a number of auxiliary objects: 

- the renderers used (with their data sources attached), 
- the clock (or master and slave clock), 
- probably more.