Ambulant design, overall
========================

Last updated for Ambulant version 0.3.

Design process
--------------

The design is going to consist of a mixed bag of technologies:

- Text files such as this one for informal descriptions. They are marked
  up as reStructuredText_, a format easily converted to HTML
  but also readable in source form.
- C++ header files for class definitions and such.
- Image files for UML pictures.
- Formal text descriptions for each design entity as prescribed by the IEEE STD 1016-1998 
  and explained in sdd-schema.txt. *Note:* this has not been done yet.
  
.. _reStructuredText: http://docutils.sourceforge.net/rst.html
  
If possible all design documents will carry a notice stating for
which version of Ambulant they were last updated, so it is relatively
easy to spot outdated (or potentially outdated) documents.
  
When still applicable design choices that were tried and rejected will
remain listed, with the reason for their rejection. This within the limits or
reasonability:-).

Open issues and discussion points are indented paragraphs starting with
an emphasized upper case word, as in

	*QUESTION*: explanation of an issue that needs to be resolved.
  
The design is going to be done in C++, but we will try to keep it compatible
with Java or Python, and if possible keep the option of cross-language implementations
open.

Source code organisation
------------------------

Everything is kept under CVS. On Unix we use the standard *automake*,
*autoconf*, *configure* and *gcc* (version 3.2 or later) toolset to
build things. On Windows we use Visual Studio 7. For cross-compilation
for Windows CE we use Embedded Visual C++ 3.0 (correct?). For
cross-compilation for the Zaurus Linux handheld we use the Sharp
toolset, based on gcc 2.95.

At the toplevel we have a number of subdirectories:

- ``Documentation`` will eventually contain all documentation, currently
  it only has ``Design``, which is what you are reading now.
- ``include`` has all the C++ header files.
- ``src`` has all the sources, with the engine built as a library
  from the ``libambulant`` subdirectory, and driver programs for the
  various platforms (not in the current version).
- ``Tests`` has test code. The only subdirectory currently in active use
  is ``designtests`` which has a program that does minimal testing of all
  modules.
- ``third_party_packages`` holds the source to third party packages we use,
  currently only James Clark's ``expat`` XML parser.

Source code conventions
-----------------------

Here's a somewhat random list of source code conventions that we have decided
to use:

- Indent 4 spaces, with the following exceptions:

  - ``namespace`` doesn't indent at all
  - ``public:``, ``private:`` and such indent two spaces
  
- Whether opening braces are at end-of-line or beginning of the next
  line depends on the circumstances and personal taste. Closing braces,
  when on a line by themselves, must however align with the construct
  that opened them.
  
- no camelCase, CamelCase or Capitalization in class or variable names
- Underscores to delimit words
- attribute names start with "m\_"
- we have a toplevel namespace ambulant, with a second level of namespaces
  under that
- semi-private classes go into into a namespace named "detail".
- Template type parameters start with an upper case letter
  (as in: ``template <class A> {}``)
- Header files need to include any header files on which they depend, and
  they guard against multiple inclusion with a preprocessor construct.
  
	I think we should use the guards only in the files themselves. There
	are some places now that also check for the guards before doing the
	#include, but this seems like overkill, and moreover makes the code less readable.
	
- Header files are all in an "ambulant" directory, and are included by full
  path, as in::
  
  	#include "ambulant/net/url.h"
  	
- Everything goes into namespace "ambulant", with sub-namespaces "lib",
  "net", etc. Machine-dependent code goes into it's own "unix", "win32", etc
  subnamespace of those.
- Source files have ``using namespace ambulant;`` at the top. In addition,
  there's a using namespace for your own namespace, i.e. ``using namespace common;``
  for source files in common. Normally, there are no other global ``using namespace``
  declarations, i.e. everything outside of your own namespace is used qualified.
- Header files that declare abstract interfaces try to include as few other
  header files as possible. In other words, if you need a ``lib::node *`` in an
  abstract header file it is better not to include ``ambulant/lib/node.h``,
  but instead to add a construct::
  
	namespace ambulant {
	namespace lib {
	class node;
	}
	}
  
  
	
- We need to define which preprocessor defines we are going to
  switch on for platform-dependent code, so we don't get `#ifdef _WIN32` in
  one place and something completely different in another place.
  Suggestion:
  
  - `#ifdef _WIN32` to test for Windows
  - something to test for Linux
  - `#ifdef __APPLE__` to test for MacOSX
  - something to test for 386 or other architectures?
  - something to test for Visual Studio versus gcc?
  
Machine-dependent code
----------------------

The general way to handle machine-dependency is to create a machine-independent
abstract base class, plus machine-dependent subclasses. Then there is a factory
function that creates a machine-dependent instance and returns it casted
to its machine-independent base class.

With this scheme we can handle machine-dependent extensions to the base class
easily: modules using these extensions declare objects of the subclass and
call the initializer directly in stead of through the factory function.

The scheme does not work for all objects, however: it breaks if we want to
create static copies of the objects. For classes for which this is the case,
such as the ``critical_region`` object, we declare an abstract object 
``abstract_critical_section`` in
``lib/abstract_mtsync.h``, subclass that as ``PLATFORM::critical_section``
in ``lib/PLATFORM/PLATFORM_mtsync.h``,
conditionally include that in ``lib/mtsync.h`` and create an empty subclass
``critical_section`` of it. 

Global structure
----------------

We want to be able to use the code
to create, say, a plugin SMIL renderer for use in a browser we need a global
"playback engine" object that has all the others hanging off it (plus factory
methods to create them, etc). To support this we have a global object ``player``
that is the controller of all aspects of playback of a single SMIL document.

With a structure like this the application itself becomes basically a skeleton
embedder: it is responsible for the GUI, handling open/open URL/quit/etc, it creates
playback engine objects when needed and has a small number of callbacks for
"create window" and such.

We have a number of objects that come in active and passive
flavors (data sources, data sinks). For the time being these do not have a common
base class but in stead follow an informal protocol, and the example objects
``passive_skeleton`` and ``active_skeleton`` can be used as a boilerplate to implement
new objects following this protocol. We may decide to use a common base class in
the future.  

run-time system
---------------

Because we have a C++ implementation
we cannot rely on refcounting or garbage collection in the underlying runtime.
lib/refcount.h has a simple refcounting implementation that is used for
garbage collection.

We need to decide on a per-case basis whether we need refcounting or whether auto_ptr
or plain pointers are more appropriate. Here's some prose by KK that gives some
of the considerations:

	I think that ref-counting should be used when it is absolutely needed
	(proven to be needed). It adds an overhead and some complexity since it can
	not happen automatically. On the other hand there are some cases where it
	simplifies the code  a lot. If I judge from my code and the code of others I
	have seen, only very few objects need ref counting. First, objects owned by
	a class, and quite all are, never need to be ref counted. You either use
	auto_ptr or just delete them. The auto_ptr may be used more extensively but
	also most of the times plain pointers can do the job. I think that
	ref-counting is needed when objects containing references are shared by
	completely independent components. For example you have a processor object
	with a queue and other application objects submit work items to this queue
	AND the work items contain references to objects that must be available when
	the work item is executed. If the work items are self-contained or are
	shallow enough to make them self-contained, then you don't need
	ref-counting.

The architecture is fairly tightly coupled. The original idea
of allowing the high-level scheduler to live on a different machine, precomputing
schedules and ending these to a low level scheduler, isn't going to work
for SMIL without putting almost all SMIL complexity in the low-level scheduler.

main loop
---------

The basic architecture is going to be event-driven, with a small
number of worker threads picking up events from the event queue. The alternative
is to use multiple threads all over, but it seems event-driven is the better
choice. An object that wants to use multiple threads can do so more easily
on an event infrastructure than the other way around, but these threads are
"somebody else's problem", as they are hidden from the rest of the architecture.

In the future, we may want to allow the option (probably compile time) to allow only a single worker
threads to handle the events. This should be a compile time option, so we don't
incur the overhead of locking the event mgr data structures if there's only a
single worker thread. For now the code assumes multiple threads.

At first glance that it appears some objects, such as a renderer,
would benefit from a threaded architecture it turns out this isn't really so.
The naive threaded implementation::

    while data = read_data():
    	render_data(data)

will not work, because many other things can also happen, such as a user-initiated
event, or the timeline for the renderer being torn down. So, the naive loop
sketched here will become hairy anyway, and look like::

	while event = wait_for_some_interesting_event():
		switch event:
			case DATA: render_data(data)
			case STOP: close_resources_and_exit()
			...

so we might as will split this out in the architecture.

The event handler architecture needs an elaborate priority scheme, that is expressive
enough that the best execution order of things that happen "at the same time"
is automatic.

If possible we should design the data structures and API such that a shortcut can be
taken (if A schedules a callback for B and nothing of higher priority is runnable then
we call B directly in stead of going through the callback main loop).

integrating third-party tools
-----------------------------

We need to be able to use existing toolkits that take work out of our hands.
Think of QuickTime and DirectX, where you basically pass a URL and say "play" and
have nothing to worry about anymore. Also, existing URL access libraries (such
as the caching infrastructure on windows) and a third party RTSP library need
to be used. This also means we don't have to handle firewalls and what more.
Investigating toolkits we can use needs to be done early, as it may influence
some of the other design issues (GC, event model).

remarks.txt lists options we have for XML parsers, SAX/DOM, regex and RTP/RTSP toolkits.

We need to be able to re-use existing (Explorer, Netscape) plugins, when applicable.

scheduler
---------

The scheduler is going to be split in two. There is a high-level language
dependent scheduler and a low-level language independent one. We will write a SMIL
high level scheduler, other people should be able to replace that with, say,
a QuickTime or MPEG-4 scheduler while re-using the low-level scheduler.
For the first release we will supply a scheduler for MMS 2.0. This is a very
small subset of SMIL 2.0, with documents following a well-defined structure
and naming convention.

The initial highlevel scheduler handles the 3GPP MMS 2.0 subset of SMIL
only, and is wholly contained in the ``timeline_builder`` object.

The Low-level scheduler is based on fixed timelines. The HL
scheduler identifies a portion of the SMIL document that it can fit on a fixed
timeline (at least: a fully ordered linear timeline), it creates a schedule for this
and pushes it down. These low-level schedules look somewhat like a petri-net,
but they probably need "or" in addition to "and". The LL schedule will contain
events that trigger the HL scheduler when needed (such as when the user clicks
a link, or when the schedule finishes). These timelines are what I referred to
in the past as "multimedia basic blocks". A description of how these timelines
work is in ``timeline.txt``.

Eventually, the elements in the timeline need enough information to be able to get back at
a HL object from them, so we do the annotation thing. Also, we need to be able
to get from the HL object tree to the timeline objects too, so that hyperjumps
can clear out the relevant timelines. 

Timeline schedules are completely independent, at their level. If one timeline
kills or pauses another one this happens because it sends an event to the
HL scheduler which does the killing or pausing. It may be that timelines can
share a clock (or, probably better, that the clock in a timeline can be slaved
to the clock in another timeline).

I think there are always going to be situations where we want to "restart the world",
i.e. stop everything, tell the highlevel scheduler where we are and let it re-create
the lowlevel schedules to put us back where we were before the restart (but respecting
the new situation that caused the restart). Situations that come to mind are a resize
of the playback window, UI changes like turning on subtitles or other system-test
attributes, etc. We could probably solve each of these cases without restarting,
but if we have the mechanism to do a restart (and do it without too much impact
on the user experience) it can be used as a fallback mechanism, and speed up
initial implementation.

Coupling between high and low level
-----------------------------------

This section is here because I don't know where else to put it. Something that
struck me is that there are a couple of objects that are really one-one mappings
of SMIL nodes at different levels. Specifically, the renderer object is the SMIL
node at the timeline level. Also, it seems that if we decide to always create
a subregion when playing a node there might also be a 1-1 mapping here.

Could we somehow take these three objects and make them "live together" for
efficiency reasons? What I would like is three distinct interfaces but all living
in the same object. At least, optionally: if we at some point decide to do
the decoupled player the objects will become distinct.

But maybe we are good enough off initially by specifying in the UML that this
1-1 relationship exists, and put off any optimization until later.


Missing
-------

Here is a list of bits of the design that I know are missing:

- What events are accepted by what objects, and what the arguments are.
- How we handle anchors. Maybe treat as nodes? 
- How we handle transitions. Maybe also treat as nodes? We probably shouldn't
  follow the SMIL model too closely, as its transitions are a bit funny.
- How we handle subregions. Maybe the same as normal regions, but created dynamically?
  Or are they simply a part of the renderer objects?
  Does this influence anchors too? Do regions belong to a timeline?
- How we handle SMIL animation. This needs to be done in a way that doesn't
  require restarting the world.