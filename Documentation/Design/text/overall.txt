Ambulant design, overall
========================
Jack Jansen, 21-May-03
----------------------

Here are some general (unstructured) thoughts on the Ambulant design.

First and foremost, we need to decide what we are going to use to describe the
design. On the one hand I'm pretty fond of informal methods, such as this
document, which is then augmented with C++ header files or other ways to
specify the interface. At some point we will need more formal descriptions,
but the question is at which stage of the project we should introduce these.
I think KK has experience with more design specs, let's see what he has to say. 

It would be nice if we could make the design language-independent, and
do it so that cross-language implementations are possible. This would not
only benefit people extending it in the long run, but could also jump-start
the initial implementation by re-using the existing GRiNS scheduler.

Boost.python may be a way to achieve C++/Python interoperability, this needs
to be investigated. Maybe Kleanthis knows more.
	
	XXXX Turns out this is not a good idea, Boost.python is big and doesn't
	really do what we want. So we just have to define the APIs clearly.

A point of concern is garbage collection. Because we want to do a C++ implementation
we cannot rely on refcounting or garbage collection (I think? Maybe there
is a C++ library or template or whatever that can handle this for us?)
so we have to be careful of object ownership and cleanup.

	It seems that a variation on the STL autoptr is possible. We need to ask
	KK for details on this (or other insight he has), and we also need to check
	that this doesn't impact performance on handhelds overmuch.
	
	XXXX Turns out this is indeed feasible.
	
The cross-language and garbage collection paragraphs may bite each other.

The architecture is going to be fairly tightly coupled. The original idea
of allowing the high-level scheduler to live on a different machine, precomputing
schedules and ending these to a low level scheduler, isn't going to work
for SMIL without putting almost all SMIL complexity in the low-level scheduler.

	XXXX Outcome of the May 21 meeting is that we should be able to do the design
	in such a way that we can allow for a split. For this, we need to de-couple
	the HL-scheduler from the event processor and run it in a separate thread.
	The only communication between HL-scheduler and the rest of the system are
	downcalls to install timelines and upcalls to signal that user interaction has
	happened. What we do need to investigate is whether it is then still possible
	to use the Data Source objects from the scheduler, and/or whether special
	measures are needed for that.

The basic architecture is going to be event-driven. The alternative
is to use multiple threads all over, but it seems event-driven is the better
choice. An object that wants to use multiple threads can do so more easily
on an event infrastructure than the other way around, but these threads are
"somebody else's problem", as they are hidden from the rest of the architecture.

	XXXX We do want to allow the option (probably compile time) to allow multiple worker
	threads to handle the events. This should be a compile time option, so we don't
	incur the overhead of locking the event mgr data structures if there's only a
	single worker thread.

Also, although it seems at first glance that some objects, such as a renderer,
would benefit from a threaded architecture it turns out this isn't really so.
The naive threaded implementation::

    while data = read_data():
    	render_data(data)

will not work, because many other things can also happen, such as a user-initiated
event, or the timeline for the renderer being torn down. So, the naive loop
sketched here will become hairy anyway, and look like::

	while event = wait_for_some_interesting_event():
		switch event:
			case DATA: render_data(data)
			case STOP: close_resources_and_exit()
			...

so we might as will split this out in the architecture.

The event handler architecture needs an elaborate priority scheme, that is expressive
enough that the best execution order of things that happen "at the same time"
is automatic.

	A problem with the event handler is how to represent an event callback.
	Ideally (in Python) it would be a tuple (anobject, amethod, arguments) and
	it would call "anobject->amethod(arguments)", but this is impossible in C++.
	The equivalent (aboundmethod, arguments) also doesn't work, as C++ apparently
	cannot represent a bound method. There may be a solution in using a common
	base class CallbackHandler that declares virtual methods for all callback
	types.
	
	XXXX Turns out this is doable, KK sent me sample code that I still need to investigave.

We need to be able to re-use existing (Explorer, Netscape) plugins, when applicable.

We need to be able to use existing toolkits that take work out of our hands.
Think of QuickTime and DirectX, where you basically pass a URL and say "play" and
have nothing to worry about anymore. Also, existing URL access libraries (such
as the caching infrastructure on windows) and a third party RTSP library need
to be used. This also means we don't have to handle firewalls and what more.
Investigating toolkits we can use needs to be done early, as it may influence
some of the other design issues (GC, event model).

The scheduler is going to be split in two. There is a high-level language
dependent scheduler and a low-level independent one. We will write a SMIL
high level scheduler, other people should be able to replace that with, say,
a QuickTime or MPEG-4 scheduler while re-using the low-level scheduler.

	XXXX The description of the LL-scheduler needs much more work, there is a
	lot of implicit knowledge Sjoerd and Jack have that isn't in here. To be done
	shortly. The wording also needs to be changed to be more object-oriented,
	i.e. refer to "timeline objects" in stead of "the low level scheduler".
	
The Low-level scheduler is going to be based on fixed timelines. The HL
scheduler identifies a portion of the SMIL document that it can fit on a fixed
timeline (at least: a fully ordered linear timeline), it creates a schedule for this
and pushes it down. These low-level schedules look somewhat like a petri-net,
but they probably need "or" in addition to "and". The LL schedule will contain
events that trigger the HL scheduler when needed (such as when the user clicks
a link, or when the schedule finishes). These timelines are what I referred to
in the past as "multimedia basic blocks".

The elements in the timeline need enough information to be able to get back at
a HL object from them, so we do the annotation thing. Also, we need to be able
to get from the HL object tree to the timeline objects too, so that hyperjumps
can clear out the relevant timelines. 

Timeline schedules are completely independent, at their level. If one timeline
kills or pauses another one this happens because it sends an event to the
HL scheduler which does the killing or pausing. It may be that timelines can
share a clock (or, probably better, that the clock in a timeline can be slaved
to the clock in another timeline).

I think there are always going to be situations where we want to "restart the world",
i.e. stop everything, tell the highlevel scheduler where we are and let it re-create
the lowlevel schedules to put us back where we were before the restart (but respecting
the new situation that caused the restart). Situations that come to mind are a resize
of the playback window, UI changes like turing on subtitles or other system-test
attributes, etc. We could probably solve each of these cases without restarting,
but if we have the mechanism to do a restart (and do it without too much impact
on the user experience) it can be used as a fallback mechanism, and speed up
initial implementation.

Missing
-------

Here is a list of bits of the design that I know are missing:

- What an event looks like. In a dynamic language we could use a bound method
  plus an optional argument, but this won't work in C++ (I think?).
- What events are accepted by what objects, and what the arguments are.
- How we handle anchors. Maybe treat as nodes? 
- How we handle transitions. Maybe also treat as nodes? We probably shouldn't
  follow the SMIL model too closely, as its transitions are a bit funny.
- How we handle subregions. Maybe the same as normal regions, but created dynamically?
  Or are they simply a part of the renderer objects?
  Does this influence anchors too? Do regions belong to a timeline?
- How we handle SMIL animation. This needs to be done in a way that doesn't
  require restarting the world.